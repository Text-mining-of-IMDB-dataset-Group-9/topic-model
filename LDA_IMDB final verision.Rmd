---
title: "fidelity LDA"
author: "Lintong Li, Jiahao Liu，Kaiwei Xiao, Zijia Wang"
date: "2022-11-12"
output:
  word_document: default
  pdf_document: default
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r}
library(tidyverse)
library(tidytext)
library(janeaustenr)
library(stringr)
library(ggplot2)
library(topicmodels)
library(tm)
## Import the IMDB data, and transfer the type of dataset as tibble.
IMDB.Dataset <- read_csv("IMDB Dataset.csv", show_col_types = F)
IMDB <- tibble(IMDB.Dataset)

## Add IMDB data with column docs by using mutate() function.
IMDB <- IMDB  %>%  mutate(docs = c(1:length(IMDB$review)))

## In order to better analysis our IMDB review words, we apply stop words to delete the words that might distract our evaluation.

## Import stop words data set.
data(stop_words)
new = stop_words$word
new = data.frame(new)

## Combining with our IMDB data set, we use rbind() function to add these specific stop words, which are meaningless or repeated, into stop_word data.
stop_w = rbind(new,"movie", "film", "films", "movies", "acting", "act", "role",
               "actor", "actors", "scenes","scene", "character","br","cast", 
               "characters", "make", "director", "10", "watch","watching", "2")
colnames(stop_w) <- c("word")
#stop_words <- rbind(stop_words,c("br","Smart" ))
#stop_words = c(,"my","custom","words")
```

```{r}
##tf-idf
book_words <- IMDB %>%
  unnest_tokens(word, review) %>%
  anti_join(stop_w)%>%
  anti_join(stop_words)%>%
  count(docs, word, sort = TRUE)

## We calculate the total words in each novel here, for later use.
total_words <- book_words %>%
  group_by(docs) %>% 
  summarize(total = sum(n))

book_words <- left_join(book_words, total_words)

## Then use row_number() to find the rank and rank column here tells us the rank of each word within the frequency table.
freq_by_rank <- book_words %>% 
  group_by(docs) %>% 
  mutate(rank = row_number(), 
         `term frequency` = n/total) %>%
  ungroup()

## First we look at term frequency (tf), which means how frequently a word occurs in a document.

## And term’s inverse document frequency (idf), which decreases the weight for commonly used words and increases the weight for words that are not used very much in a collection of documents. 

## Thirdly, combining with term frequency to calculate a term’s tf-idf (the two quantities multiplied together), the frequency of a term adjusted for how rarely it is used.
book_tf_idf <- book_words %>%
  bind_tf_idf(word, docs, n)

## We want delete the uncommon words that hardly use in this document. So, select the words only with the term frequency is larger than 0.1.
book_tf_idf_new <- filter(book_tf_idf, tf > 0.1)

## We look at the distribution of term frequency(tf), n/total for each novel, the number of times a word appears in a novel divided by the total number of terms (words) in that novel.
ggplot(book_tf_idf_new, aes(tf, fill = docs)) +
  geom_histogram(show.legend = FALSE, color = 'black', fill = 'light blue') + geom_density(alpha=.2, fill="#FF6666") + labs(title = 'Density of word tf bigger than 0.1')
```

First, we plot the distribution of term frequency which is larger than 0.1, and we found that there is a very long tail to the right of this novel which means existing those extremely rare words!

```{r}
## We use count() function to find the most common words.
df <- book_tf_idf_new %>% 
  group_by(word) %>% 
  count(sort = TRUE)

## We likely want to change all of the keywords to either lower or upper case to get rid of duplicates like “MOVIE” and “Movie”. 
df <- df %>% 
  mutate(word = toupper(word))

##Then We use pairwise_count() from the widyr package to count how many times each pair of words occurs together in a title or description field.
library(widyr)
word_pairs <- book_words %>% 
  pairwise_count(word, docs, sort = TRUE, upper = FALSE)
```

The idea of tf-idf is to find the important words for the content of each document by decreasing the weight for commonly used words and increasing the weight for words that are not used very much in a collection or corpus of documents.

```{r}
## We use ggplot package to plot the 15 most common words in this review documents.
df_new <- head(df,15)
ggplot(df_new, aes(x = n, fill = word)) + geom_histogram() + labs(title = 'Top 15 Most Frequent Words in Reviews')
```

From the Top 15 Most Frequent Words in Reviews table, we get these words are ACTION, BAD, FUNNY, GAME, HORROR, LOVE, MUSIC, PEOPLE, PLOT, SERIES, STORY, TERRIBLE, TIME, and WORST. From above words, we could give a guess that people probably like watching movies about action, funny, game, love, music and so on. They watch movies with the aime at entertaining rather than learning something new. Or other words, to kill the time, which is also displayed in the Top 15 Most Frequent Words in Reviews table.

```{r}
## We will again use the ggraph package for visualizing our networks. We plot networks of these co-occurring words so we can see these relationships better.
library(igraph)
library(ggraph)
set.seed(1234)
word_pairs %>%
  filter(n >= 1800) %>%
  graph_from_data_frame() %>%
  ggraph(layout = "fr") +
  geom_edge_link(aes(edge_alpha = n, edge_width = n), edge_colour = "cyan4") +
  geom_node_point(size = 5) +
  geom_node_text(aes(label = name), repel = TRUE, 
                 point.padding = unit(0.2, "lines")) +
  theme_void()
```

We see some clear clustering in this network of title words; word "time" is at center and is largely organized into several families of words that tend to go together. It is largely proved our guess that people watching movies is aimed at killing their leisure time, so they are prone to these relaxing types of movies.

```{r fig.width=6, fig.height=4}
###  Latent Dirichlet allocation.
imdb_dtm <- IMDB %>%
  unnest_tokens(word, review) %>%
  anti_join(stop_w)%>%
  count(docs, word) %>%
  cast_dtm(docs, word, n)
```

```{r}
## set a seed so that the output of the model is predictable
## A LDA_VEM topic model with 2 topics.
ap_lda <- LDA(imdb_dtm, k = 2, control = list(seed = 1234))

##The tidytext package provides this method for extracting the per-topic-per-word probabilities, called (“beta”), from the model.
ap_topics <- tidy(ap_lda, matrix = "beta")

## We use dplyr’s slice_max() to find the 10 terms that are most common within each topic. As a tidy data frame, this lends itself well to a ggplot2 visualization.
ap_top_terms <- ap_topics %>%
  group_by(topic) %>%
  slice_max(beta, n = 10) %>% 
  ungroup() %>%
  arrange(topic, -beta)

ap_top_terms %>%
  mutate(term = reorder_within(term, beta, topic)) %>%
  ggplot(aes(beta, term, fill = factor(topic))) +
  geom_col(show.legend = FALSE) +
  facet_wrap(~ topic, scales = "free", ncol = 5) +
  scale_y_reordered() 
```

From above Top Term words table 1, we can see that the first movie contains words like story, love and life. We can infer that the type od first movie is family drama or story, which is suitable for families to watch together. However, Top Term words table 2, it contains word "horror" in the second movie, we guess it was a horror movie.

```{r}
library(tidyr)
library(hrbrthemes)

ap_topics <- tidy(ap_lda, matrix = "beta")
ap_topics

## Consider the terms that had the greatest difference in beta between topic 1 and topic 2. This can be estimated based on the log ratio of the two.
## We visualize the words with the greatest differences between the two topics.
beta_wide <- ap_topics %>%
  mutate(topic = paste0("topic", topic)) %>%
  pivot_wider(names_from = topic, values_from = beta) %>% 
  filter(topic1 > .001 | topic2 > .001) %>%
  mutate(log_ratio = log2(topic2 / topic1))

head(beta_wide)

ggplot(data = beta_wide, aes(x = x)) + geom_density( aes(x = topic1 , y = ..density..), fill="#69b3a2" ) + geom_label( aes(x = 0.01, y = 500, label="Topic1"), color="#69b3a2") +
  # Bottom
  geom_density( aes(x = topic2, y = -..density..), fill= "#404080") +
  geom_label( aes(x = 0.01, y = 500,label="Topic2"), color="#404080") +
  theme_ipsum() +
  xlab("value of x")
```

From the density difference visulization plot, we get that in Topic 2, it is more concertrated at a word and has a higher density, compared with the Topic 1.

```{r}
beta_wide %>%
  group_by(direction = log_ratio > 0) %>%
  slice_max(abs(log_ratio), n = 10) %>% 
  ungroup() %>%
  mutate(term = reorder(term, log_ratio)) %>%
  ggplot(aes(log_ratio, term)) +
  geom_col() +
  labs(x = "Log2 ratio of beta in topic 2 / topic 1", y = NULL)
```

```{r fig.width=6, fig.height=4}
## We then use the LDA() function to create a ten-topic model. 
ap_lda <- LDA(imdb_dtm, k = 10, control = list(seed = 1234))

ap_topics <- tidy(ap_lda, matrix = "beta")
## We use dplyr’s slice_max() to find the top 10 terms within each topic
ap_top_terms <- ap_topics %>%
  group_by(topic) %>%
  slice_max(beta, n = 10) %>% 
  ungroup() %>%
  arrange(topic, -beta)
## This tidy output lends itself well to a ggplot2 visualization
ap_top_terms %>%
  mutate(term = reorder_within(term, beta, topic)) %>%
  ggplot(aes(beta, term, fill = factor(topic))) +
  geom_col(show.legend = FALSE) +
  facet_wrap(~ topic, scales = "free", ncol = 5) +
  scale_y_reordered() 
```

We get 10 topic top 10 words. Most of them contain words like time, love, life and so on. The β tells us the probability of that term being generated from that topic for that document. It is the probability of that term (word) belonging to that topic. Notice that some of the values for β are very, very low, and some are not so low.

```{r}
tidy_lda <- tidy(ap_lda)
tidy_lda
```

Let's examine the top 10 terms for each topic.

```{r}
top_terms <- tidy_lda %>%
  group_by(topic) %>%
  slice_max(beta, n = 10, with_ties = FALSE) %>%
  ungroup() %>%
  arrange(topic, -beta)

top_terms
```

```{r fig.width=6, fig.height=4}
top_terms %>%
  mutate(term = reorder_within(term, beta, topic)) %>%
  group_by(topic, term) %>%    
  arrange(desc(beta)) %>%  
  ungroup() %>%
  ggplot(aes(beta, term, fill = as.factor(topic))) +
  geom_col(show.legend = FALSE) +
  scale_y_reordered() +
  labs(title = "Top 10 terms in each LDA topic",
       x = expression(beta), y = NULL) +
  facet_wrap(~ topic, ncol = 5, scales = "free")
```

```{r}
## We want to know which topics are associated with each document. We can find this by examining the per-document-per-topic probabilities (“gamma”).
lda_gamma <- tidy(ap_lda, matrix = "gamma")

## First we visualize for all topic and then we visualize the per-document-per-topic probability for each topic.
ggplot(lda_gamma, aes(gamma)) +
  geom_histogram(alpha = 0.8, col = 'black', fill = 'light blue') + 
  scale_y_log10() +
  labs(title = "Distribution of probabilities for all topics",
       y = "Number of documents", x = expression(gamma))

```

```{r fig.width=8, fig.height=8}
ggplot(lda_gamma, aes(gamma, fill = as.factor(topic))) +
  geom_histogram(alpha = 0.8, show.legend = FALSE) +
  facet_wrap(~ topic, ncol = 4) +
  scale_y_log10() +
  labs(title = "Distribution of probability for each topic",
       y = "Number of documents", x = expression(gamma))
```

From the each topic visulization plots, we know that in each topic, there are similar distribution.
