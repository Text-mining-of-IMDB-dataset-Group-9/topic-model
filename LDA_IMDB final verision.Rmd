---
title: "fidelity LDA"
author: "Lintong Li, Jiahao Liu，Kaiwei Xiao, Zijia Wang"
date: "2022-11-12"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r}
library(tidyverse)
library(tidytext)
library(janeaustenr)
library(stringr)
library(ggplot2)
library(topicmodels)
library(tm)
## Import the IMDB data, and transfer the type of dataset as tibble.
IMDB.Dataset <- read_csv("IMDB Dataset.csv", show_col_types = F)
IMDB <- tibble(IMDB.Dataset)

## Add IMDB data with column docs by using mutate() function.
IMDB <- IMDB  %>%  mutate(docs = c(1:length(IMDB$review)))

## In order to better analysis our IMDB review words, we apply stop words to delete the words that might distract our evaluation.

## Import stop words data set.
data(stop_words)
new = stop_words$word
new = data.frame(new)

## Combining with our IMDB data set, we use rbind() function to add these specific stop words, which are meaningless or repeated, into stop_word data.
stop_w = rbind(new,"movie", "film", "films", "movies", "acting", "act", "role",
               "actor", "actors", "scenes","scene", "character","br","cast", 
               "characters", "make", "director", "10", "watch","watching", "2")
colnames(stop_w) <- c("word")
#stop_words <- rbind(stop_words,c("br","Smart" ))
#stop_words = c(,"my","custom","words")
```



```{r}
##tf-idf
book_words <- IMDB %>%
  unnest_tokens(word, review) %>%
  anti_join(stop_w)%>%
  anti_join(stop_words)%>%
  count(docs, word, sort = TRUE)

## We calculate the total words in each novel here, for later use.
total_words <- book_words %>%
  group_by(docs) %>% 
  summarize(total = sum(n))

book_words <- left_join(book_words, total_words)

## Then use row_number() to find the rank and rank column here tells us the rank of each word within the frequency table.
freq_by_rank <- book_words %>% 
  group_by(docs) %>% 
  mutate(rank = row_number(), 
         `term frequency` = n/total) %>%
  ungroup()

## First we look at term frequency (tf), which means how frequently a word occurs in a document.

## And term’s inverse document frequency (idf), which decreases the weight for commonly used words and increases the weight for words that are not used very much in a collection of documents. 

## Thirdly, combining with term frequency to calculate a term’s tf-idf (the two quantities multiplied together), the frequency of a term adjusted for how rarely it is used.
book_tf_idf <- book_words %>%
  bind_tf_idf(word, docs, n)

## We want delete the uncommon words that hardly use in this document. So, select the words only with the term frequency is larger than 0.1.
book_tf_idf_new <- filter(book_tf_idf, tf > 0.1)

## We look at the distribution of term frequency(tf), n/total for each novel, the number of times a word appears in a novel divided by the total number of terms (words) in that novel.
ggplot(book_tf_idf_new, aes(tf, fill = docs)) +
  geom_histogram(show.legend = FALSE, color = 'black', fill = 'light blue') + geom_density(alpha=.2, fill="#FF6666") + labs(title = 'Density of word tf bigger than 0.1')
```
## There is a very long tail to the right of this novel which means existing those extremely rare words!

```{r}
## We use count() function to find the most common words.
df <- book_tf_idf_new %>% 
  group_by(word) %>% 
  count(sort = TRUE)

## We likely want to change all of the keywords to either lower or upper case to get rid of duplicates like “MOVIE” and “Movie”. 
df <- df %>% 
  mutate(word = toupper(word))

##Then We use pairwise_count() from the widyr package to count how many times each pair of words occurs together in a title or description field.
library(widyr)
word_pairs <- book_words %>% 
  pairwise_count(word, docs, sort = TRUE, upper = FALSE)
```

```{r}
## We use ggplot package to plot the 15 most common words in this review documents.
df_new <- head(df,15)
ggplot(df_new, aes(x = n, fill = word)) + geom_histogram() + labs(title = 'Top 15 Most Frequent Words in Reviews')
```
## From the Top 15 Most Frequent Words in Reviews table, we get these words are ACTION, BAD, FUNNY, GAME, HORROR, LOVE, MUSIC, PEOPLE, PLOT, SERIES, STORY, TERRIBLE, TIME, and WORST. From above words, we could give a guess that people probably like watching movies about action, funny, game, love, music and so on. They watch movies with the aime at entertaining rather than learning something new. Or other words, to kill the time, which is also displayed in the Top 15 Most Frequent Words in Reviews table.

```{r}
## We will again use the ggraph package for visualizing our networks. We plot networks of these co-occurring words so we can see these relationships better.
library(igraph)
library(ggraph)
set.seed(1234)
word_pairs %>%
  filter(n >= 1800) %>%
  graph_from_data_frame() %>%
  ggraph(layout = "fr") +
  geom_edge_link(aes(edge_alpha = n, edge_width = n), edge_colour = "cyan4") +
  geom_node_point(size = 5) +
  geom_node_text(aes(label = name), repel = TRUE, 
                 point.padding = unit(0.2, "lines")) +
  theme_void()
```
## We see some clear clustering in this network of title words; word "time" is at center and is largely organized into several families of words that tend to go together. It is largely proved our guess that people watching movies is aimed at killing their leisure time, so they are prone to these relaxing types of movies.

```{r fig.width=6, fig.height=4}
###  Latent Dirichlet allocation.
imdb_dtm <- IMDB %>%
  unnest_tokens(word, review) %>%
  anti_join(stop_w)%>%
  count(docs, word) %>%
  cast_dtm(docs, word, n)
```

```{r}
## set a seed so that the output of the model is predictable
## A LDA_VEM topic model with 2 topics.
ap_lda <- LDA(imdb_dtm, k = 2, control = list(seed = 1234))

##The tidytext package provides this method for extracting the per-topic-per-word probabilities, called (“beta”), from the model.
ap_topics <- tidy(ap_lda, matrix = "beta")

## We use dplyr’s slice_max() to find the 10 terms that are most common within each topic. As a tidy data frame, this lends itself well to a ggplot2 visualization.
ap_top_terms <- ap_topics %>%
  group_by(topic) %>%
  slice_max(beta, n = 10) %>% 
  ungroup() %>%
  arrange(topic, -beta)

ap_top_terms %>%
  mutate(term = reorder_within(term, beta, topic)) %>%
  ggplot(aes(beta, term, fill = factor(topic))) +
  geom_col(show.legend = FALSE) +
  facet_wrap(~ topic, scales = "free", ncol = 5) +
  scale_y_reordered() 
```
```{r}
library(tidyr)
library(hrbrthemes)

ap_topics <- tidy(ap_lda, matrix = "beta")
ap_topics

beta_wide <- ap_topics %>%
  mutate(topic = paste0("topic", topic)) %>%
  pivot_wider(names_from = topic, values_from = beta) %>% 
  filter(topic1 > .001 | topic2 > .001) %>%
  mutate(log_ratio = log2(topic2 / topic1))

head(beta_wide)

ggplot(data = beta_wide, aes(x = x)) + geom_density( aes(x = topic1 , y = ..density..), fill="#69b3a2" ) + geom_label( aes(x = 0.01, y = 500, label="Topic1"), color="#69b3a2") +
  # Bottom
  geom_density( aes(x = topic2, y = -..density..), fill= "#404080") +
  geom_label( aes(x = 0.01, y = 500,label="Topic2"), color="#404080") +
  theme_ipsum() +
  xlab("value of x")
```

```{r}
beta_wide %>%
  group_by(direction = log_ratio > 0) %>%
  slice_max(abs(log_ratio), n = 10) %>% 
  ungroup() %>%
  mutate(term = reorder(term, log_ratio)) %>%
  ggplot(aes(log_ratio, term)) +
  geom_col() +
  labs(x = "Log2 ratio of beta in topic 2 / topic 1", y = NULL)
```

```{r fig.width=6, fig.height=4}
ap_lda <- LDA(imdb_dtm, k = 10, control = list(seed = 1234))

ap_topics <- tidy(ap_lda, matrix = "beta")

ap_top_terms <- ap_topics %>%
  group_by(topic) %>%
  slice_max(beta, n = 10) %>% 
  ungroup() %>%
  arrange(topic, -beta)

ap_top_terms %>%
  mutate(term = reorder_within(term, beta, topic)) %>%
  ggplot(aes(beta, term, fill = factor(topic))) +
  geom_col(show.legend = FALSE) +
  facet_wrap(~ topic, scales = "free", ncol = 5) +
  scale_y_reordered() 
###
```

```{r}
tidy_lda <- tidy(ap_lda)
tidy_lda
```

##Let's examine the top 10 terms for each topic.

```{r}
top_terms <- tidy_lda %>%
  group_by(topic) %>%
  slice_max(beta, n = 10, with_ties = FALSE) %>%
  ungroup() %>%
  arrange(topic, -beta)

top_terms
```



```{r fig.width=6, fig.height=4}
top_terms %>%
  mutate(term = reorder_within(term, beta, topic)) %>%
  group_by(topic, term) %>%    
  arrange(desc(beta)) %>%  
  ungroup() %>%
  ggplot(aes(beta, term, fill = as.factor(topic))) +
  geom_col(show.legend = FALSE) +
  scale_y_reordered() +
  labs(title = "Top 10 terms in each LDA topic",
       x = expression(beta), y = NULL) +
  facet_wrap(~ topic, ncol = 5, scales = "free")
```

```{r}
lda_gamma <- tidy(ap_lda, matrix = "gamma")

ggplot(lda_gamma, aes(gamma)) +
  geom_histogram(alpha = 0.8, col = 'black', fill = 'light blue') + 
  scale_y_log10() +
  labs(title = "Distribution of probabilities for all topics",
       y = "Number of documents", x = expression(gamma))

```

```{r fig.width=8, fig.height=8}
ggplot(lda_gamma, aes(gamma, fill = as.factor(topic))) +
  geom_histogram(alpha = 0.8, show.legend = FALSE) +
  facet_wrap(~ topic, ncol = 4) +
  scale_y_log10() +
  labs(title = "Distribution of probability for each topic",
       y = "Number of documents", x = expression(gamma))
```
